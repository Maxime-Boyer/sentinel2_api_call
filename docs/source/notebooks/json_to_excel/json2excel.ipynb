{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_json_xlxs.xlsx_extractor import *\n",
    "\n",
    "from utils_json_xlxs.extractor_config import ExtractorConfig\n",
    "from utils_json_xlxs.jsons_to_df import (\n",
    "    cdc_json_to_df,\n",
    "    cv_json_single_file_to_df,\n",
    "    rec_json_to_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_config_toml_path = \"/workspace/notebooks/json/2023DOS0550696/extractor_config.toml\"\n",
    "conf = ExtractorConfig(ao_config_toml_path)\n",
    "extractor = XlsxExtractor(conf)\n",
    "extractor.save_to_xlsx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROADMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "refac code [ ]  \n",
    "  \n",
    "être le plus générique possible en terme de format et/ou pouvoir faire des modifs facilement (fonction dédiée) [ ]  \n",
    "  \n",
    "/src/dag_nao/outputs/2023..696/entreprise/datetime/  \n",
    "-> les jsons et l'excel (+ document d'observabilité sur le prompt ?)\n",
    "  \n",
    "automatiser la génération (soit en auomatisant création toml soit sans passer par toml) [ ]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PIPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- genere json ao (rec, cdc) et cv\n",
    "- convert json ao to sheet 1\n",
    "- convert json cv to sheet 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_json_path = \"/workspace/notebooks/json/2023DOS0550696/amaris/rec1.json\"\n",
    "\n",
    "rec_df = rec_json_to_df(rec_json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df[\"index\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_df[\"data\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_json_path = \"/workspace/notebooks/json/2023DOS0550696/amaris/cdc1.json\"\n",
    "\n",
    "cdc_df = cdc_json_to_df(cdc_json_path)\n",
    "\n",
    "cdc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_df[\"index\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_df[\"data\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDC + REC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_df = XlsxExtractor._generate_ao_df(rec_json_path, cdc_json_path, has_metadata=True)\n",
    "\n",
    "ao_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_df[\"index\"].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AO REFAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_df_ao(rec_path: str,\n",
    "                    cdc_path: str,\n",
    "                    has_metadata: bool) -> pd.DataFrame:\n",
    "    \"\"\"Generates the SNCF AO dataframe from the SNCF JSONs and returns it\n",
    "\n",
    "    Parameters:\n",
    "        rec_path (str): path to the SNCF JSON\n",
    "        cdc_path (str): path to the CDC JSON\n",
    "\n",
    "    Returns:\n",
    "        oa_df (pd.DataFrame): AO dataframe\n",
    "    \"\"\"\n",
    "    # fetch raw dataframes from SNCF and concatenate\n",
    "    rec_df = rec_json_to_df(rec_path)\n",
    "    cdc_df = cdc_json_to_df(cdc_path)\n",
    "    \n",
    "    oa_df = pd.concat([rec_df, cdc_df], ignore_index=True)\n",
    "\n",
    "    # split metadata and actual data in separate colums\n",
    "    if has_metadata:\n",
    "        oa_df = split_metadata(oa_df)\n",
    "    return oa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_xlsx_ao(config: ExtractorConfig) -> None:\n",
    "    \"\"\"Writes JSON data to xlsx sheet\n",
    "\n",
    "    Generate the ao_dataframe, then the cv_dataframe for each profile and write to xlsx \n",
    "    sheet.\n",
    "    This class need no argument as every path is defined in the config object.\n",
    "    \"\"\"\n",
    "    # Writer config\n",
    "    writer = pd.ExcelWriter(config.XLSX_OUTNAME, engine=\"xlsxwriter\")\n",
    "    workbook = writer.book\n",
    "    formats = workbook.add_format({\"text_wrap\": True})\n",
    "\n",
    "    # Generate SNCF AO dataframe and write to xlsx sheet 1\n",
    "    ao_df = json_to_df_ao(config.REC_JSON_NAME,\n",
    "                        config.CDC_JSON_NAME,\n",
    "                        config.HAS_METADATA)\n",
    "    sheet_name = \"ao_sncf\"\n",
    "    ao_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    for column in ao_df:\n",
    "        col_idx = ao_df.columns.get_loc(column)\n",
    "        if column == \"data\":\n",
    "            writer.sheets[sheet_name].set_column(col_idx, col_idx, 60, formats)\n",
    "        else:\n",
    "            writer.sheets[sheet_name].set_column(col_idx, col_idx, 30, formats)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_config_toml_path = \"/workspace/notebooks/json/2023DOS0550696/extractor_config.toml\"\n",
    "conf = ExtractorConfig(ao_config_toml_path)\n",
    "\n",
    "json_to_df_ao(rec_path=conf.REC_JSON_NAME, cdc_path=conf.CDC_JSON_NAME, has_metadata=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_to_xlsx_ao(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OFFRE TECH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_json_path = \"/workspace/notebooks/json/2023DOS0550696/amaris/cv1.json\"\n",
    "\n",
    "cv_df = XlsxExtractor._generate_cv_df(cv_json_path, True)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OFFRE TECH REFAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cv_df(paths_to_json: str,\n",
    "                    has_metadata: bool) -> pd.DataFrame:\n",
    "    \"\"\"Generates the CV dataframe from the CV JSONs (experiences, taches, diplomes)\n",
    "\n",
    "    Some cleaning is done in this function:\n",
    "    - replace NaNs by list containing only the N/A value.\n",
    "    - flatten nested lists\n",
    "    - split metadata and actual data in separate colums\n",
    "\n",
    "    Parameters:\n",
    "        paths_to_json (str): path to the CV JSON\n",
    "    Returns:\n",
    "        cv_df (pd.DataFrame): CV dataframe\n",
    "    \"\"\"\n",
    "    # fetch raw dataframes\n",
    "    # cv_exp_df = cv_exp_json_to_df(exp_path)\n",
    "    # cv_tasks_df = cv_tasks_json_to_df(tasks_path)\n",
    "    # cv_diplomae_df = cv_diplomae_json_to_df(diplomae_path)\n",
    "    cv_exp_df, cv_tasks_df, cv_diplomae_df = cv_json_single_file_to_df(paths_to_json)\n",
    "\n",
    "    # prepare the concatenation dataframe\n",
    "    cv_df = pd.DataFrame(columns=[\"categorie\", \"index\", \"data\"])\n",
    "\n",
    "    entreprises = cv_exp_df[\"categorie\"].unique()\n",
    "\n",
    "    for e in entreprises:\n",
    "        df_1 = cv_exp_df[cv_exp_df[\"categorie\"] == e]\n",
    "\n",
    "        df_2 = cv_tasks_df[cv_tasks_df[\"categorie\"] == e]\n",
    "        df_2 = df_2.drop(df_2[df_2[\"index\"] == \"nom_entreprise\"].index)\n",
    "        df_2 = df_2.drop(df_2[df_2[\"index\"] == \"nom_poste\"].index)\n",
    "\n",
    "        df_1 = pd.concat([df_1, df_2], ignore_index=True)\n",
    "\n",
    "        cv_df = pd.concat([cv_df, df_1], ignore_index=True)\n",
    "\n",
    "    cv_df = pd.concat([cv_df, cv_diplomae_df], ignore_index=True)\n",
    "\n",
    "    # split the metadata into separate columns\n",
    "    if has_metadata:\n",
    "        cv_df = split_metadata(cv_df)\n",
    "\n",
    "    return cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_json_single_file_to_df(filename: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reads the CV JSON output and returns a dataframe\"\"\"\n",
    "    data = _load_json(filename)\n",
    "\n",
    "    # experience\n",
    "    cv_exp_df = pd.DataFrame(columns=[\"categorie\", \"index\", \"data\"])\n",
    "    for xp in data[\"experiences_entreprise\"]:\n",
    "        tmp_df = pd.json_normalize(xp).T.reset_index()\n",
    "        tmp_df.columns = [\"index\", \"data\"]\n",
    "\n",
    "        categorie_name = tmp_df[tmp_df[\"index\"] == \"nom_entreprise\"][\"data\"][0][0]\n",
    "        tmp_df[\"categorie\"] = categorie_name\n",
    "\n",
    "        cv_exp_df = pd.concat([cv_exp_df, tmp_df], ignore_index=True)\n",
    "\n",
    "    # tasks\n",
    "    cv_taches_df = pd.DataFrame(columns=[\"categorie\", \"index\", \"data\"])\n",
    "    for xp in data[\"experiences_entreprise\"]:\n",
    "        tmp_df = pd.json_normalize(xp).T.reset_index()\n",
    "        tmp_df.columns = [\"index\", \"data\"]\n",
    "\n",
    "        tmp_dg = tmp_df[tmp_df[\"index\"] == \"role\"].explode(column=\"data\")\n",
    "        tmp_df = tmp_df.drop(tmp_df[tmp_df[\"index\"] == \"role\"].index)\n",
    "        tmp_df = pd.concat([tmp_df, tmp_dg], ignore_index=True)\n",
    "\n",
    "        categorie_name = tmp_df[tmp_df[\"index\"] == \"nom_entreprise\"][\"data\"][0][0]\n",
    "        tmp_df[\"categorie\"] = categorie_name\n",
    "\n",
    "        cv_taches_df = pd.concat([cv_taches_df, tmp_df], ignore_index=True)\n",
    "\n",
    "    # degrees\n",
    "    cv_diplomes_df = pd.json_normalize(data).T.reset_index()\n",
    "    cv_diplomes_df.columns = [\"index\", \"data\"]\n",
    "\n",
    "    cv_diplomes_df = cv_diplomes_df.explode(column=\"data\")\n",
    "    cv_diplomes_df[\"categorie\"] = \"Divers (diplomes, ...)\"\n",
    "    cv_diplomes_df = cv_diplomes_df[[\"categorie\", \"index\", \"data\"]]\n",
    "    cv_diplomes_df = cv_diplomes_df[cv_diplomes_df[\"index\"] != \"experiences_entreprise\"]\n",
    "\n",
    "    return cv_exp_df, cv_taches_df, cv_diplomes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cv_path = \"/workspace/notebooks/json/2023DOS0550696/amaris/cv1.json\"\n",
    "with open(cv_path, \"rb\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(data.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = list(data.values())\n",
    "print(len(v))\n",
    "v[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['SAP Smart', {'page': 3}]\n",
    "t[-1].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "def elementary_list_check(L:list)->bool:\n",
    "    if len(L)==2:\n",
    "        if isinstance(L[-1],Dict):\n",
    "            if 'page' in L[-1].keys():\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_elementary_list(L:list):\n",
    "    return L[0], L[-1]['page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_test = v[-1][0]\n",
    "dict_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"diplomes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in dict_test.keys():\n",
    "    print(key)\n",
    "    val = dict_test[key]\n",
    "    if elementary_list_check(val):\n",
    "        info, page = split_elementary_list(val)\n",
    "    else:\n",
    "        for sub in val:\n",
    "            if elementary_list_check(sub):\n",
    "                info, page = split_elementary_list(sub)\n",
    "                print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_property_in_sheet(D:dict):\n",
    "    for key in D.keys():\n",
    "        print(key)\n",
    "        val = dict_test[key]\n",
    "        if elementary_list_check(val):\n",
    "            info, page = split_elementary_list(val)\n",
    "        else:\n",
    "            for sub in val:\n",
    "                if elementary_list_check(sub):\n",
    "                    info, page = split_elementary_list(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['SAP Smart', {'page': 3}]\n",
    "print(elementary_list_check(t))\n",
    "split_elementary_list(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_property_info_in_excel(worksheet, data, property_name, metadata=False):\n",
    "    row, col = 0, 0\n",
    "\n",
    "    for dict in data[property_name]:\n",
    "            \n",
    "            for key in dict:\n",
    "                # column 0 : name of element\n",
    "                worksheet.write(row, col, key)\n",
    "\n",
    "                val = dict[key]\n",
    "                if metadata:\n",
    "                    if elementary_list_check(val):\n",
    "                        info, page = split_elementary_list(val)\n",
    "                        worksheet.write(row, col+1, info)\n",
    "                        worksheet.write(row, col+2, page)\n",
    "                        row += 1\n",
    "\n",
    "                    else:\n",
    "                        for sub in val:\n",
    "                            if elementary_list_check(sub):\n",
    "                                info, page = split_elementary_list(sub)\n",
    "                                \n",
    "                                worksheet.write(row, col+1, info)\n",
    "                                worksheet.write(row, col+2, page)\n",
    "                                row += 1\n",
    "                \n",
    "                else:\n",
    "                    if not isinstance(val, list):\n",
    "                        worksheet.write(row, col+1, val)\n",
    "                        row += 1\n",
    "                    else:\n",
    "                        for sub in val:\n",
    "                            if not isinstance(val, Dict):\n",
    "                                worksheet.write(row, col+1, sub)\n",
    "                                row += 1\n",
    "                            else\n",
    "\n",
    "\n",
    "                        \n",
    "        row += 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance([2,4], list):\n",
    "    print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlsxwriter\n",
    "\n",
    "# open excel file\n",
    "out = \"/workspace/notebooks/outputs/hello.xlsx\"\n",
    "workbook = xlsxwriter.Workbook(out)\n",
    "\n",
    "# read json\n",
    "cv_path = \"/workspace/notebooks/json/2023DOS0550696/amaris/cv1.json\"\n",
    "with open(cv_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# keys of json ['diplomes', 'certifications', 'langues_parlees', 'experiences_entreprise']\n",
    "# properties_names = list(data.keys())\n",
    "\n",
    "\n",
    "for property_name in data.keys():\n",
    "    # create sheet\n",
    "    worksheet = workbook.add_worksheet(property_name)\n",
    "    # reset row and column indexes\n",
    "    row, col = 0, 0\n",
    "    \n",
    "    for dict in data[property_name]:\n",
    "        for key in dict:\n",
    "            print(key)\n",
    "            worksheet.write(row, col, key)\n",
    "\n",
    "            val = dict[key]\n",
    "            if elementary_list_check(val):\n",
    "                info, page = split_elementary_list(val)\n",
    "                worksheet.write(row, col+1, info)\n",
    "                worksheet.write(row, col+2, page)\n",
    "                row += 1\n",
    "\n",
    "            else:\n",
    "                for sub in val:\n",
    "                    if elementary_list_check(sub):\n",
    "                        info, page = split_elementary_list(sub)\n",
    "                        \n",
    "                        worksheet.write(row, col+1, info)\n",
    "                        worksheet.write(row, col+2, page)\n",
    "                        row += 1\n",
    "        \n",
    "        row += 5\n",
    "\n",
    "\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### claude 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cv_path = \"/workspace/notebooks/json/2022DOS1246222/amaris/formatted_json_cv.json\"\n",
    "with open(cv_path, \"rb\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiences = data[\"experiences_entreprise\"]\n",
    "diplomes = data[\"diplomes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in diplomes:\n",
    "    for key in exp.keys():\n",
    "        print(key)\n",
    "        print(type(key))\n",
    "        val = exp[key]\n",
    "        if not isinstance(val, List):\n",
    "            print(val)\n",
    "        else:\n",
    "            for sub in val:\n",
    "                print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REPONSE AO\n",
    "\n",
    "import json\n",
    "import xlsxwriter\n",
    "\n",
    "\n",
    "\n",
    "num_dos = \"2023DOS0550696\"\n",
    "entreprise = \"neurones_JMB\"\n",
    "\n",
    "# open excel file\n",
    "out = f\"/workspace/notebooks/outputs/{num_dos}/{entreprise}/reponse_ao.xlsx\"\n",
    "workbook = xlsxwriter.Workbook(out)\n",
    "\n",
    "# read json\n",
    "cv_path = f\"/workspace/notebooks/json/{num_dos}/{entreprise}/formatted_json_cv.json\"\n",
    "with open(cv_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# keys of json ['diplomes', 'certifications', 'langues_parlees', 'experiences_entreprise']\n",
    "# properties_names = list(data.keys())\n",
    "\n",
    "\n",
    "for property_name in data.keys():\n",
    "    # create sheet\n",
    "    worksheet = workbook.add_worksheet(property_name)\n",
    "    # reset row and column indexes\n",
    "    row, col = 0, 0\n",
    "    \n",
    "    for elmt in data[property_name]:\n",
    "        for key in elmt.keys():\n",
    "            print(key)\n",
    "            worksheet.write(row, col, key)\n",
    "\n",
    "            val = elmt[key]\n",
    "            if not isinstance(val, List):\n",
    "                print(val)\n",
    "                worksheet.write(row, col+1, val)\n",
    "                row += 1\n",
    "\n",
    "            else:\n",
    "                for sub in val:\n",
    "                    print(sub)\n",
    "                    worksheet.write(row, col+1, sub)\n",
    "                    row += 1\n",
    "        \n",
    "        row += 4\n",
    "\n",
    "# read json bpu\n",
    "bpu_path = f\"/workspace/notebooks/json/{num_dos}/{entreprise}/formatted_json_bpu.json\"\n",
    "with open(bpu_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "worksheet = workbook.add_worksheet(\"offre financiere\")\n",
    "row, col = 0, 0\n",
    "for k in data.keys():\n",
    "    \n",
    "    worksheet.write(row, col, k)\n",
    "    worksheet.write(row, col+1, data[k])\n",
    "    row+=1\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xlsxwriter.worksheet.Worksheet"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xlsxwriter\n",
    "out = \"/workspace/notebooks/a.xlsx\"\n",
    "workbook = xlsxwriter.Workbook(out)\n",
    "worksheet = workbook.add_worksheet(\"offre financiere\")\n",
    "type(worksheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_property_in_sheet\u001b[39m(ws:xlsxwriter\u001b[38;5;241m.\u001b[39mworksheet\u001b[38;5;241m.\u001b[39mWorksheet, property_data:\u001b[43mDict\u001b[49m, row:\u001b[38;5;28mint\u001b[39m, col:\u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m elmt_name \u001b[38;5;129;01min\u001b[39;00m property_data\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      3\u001b[0m         elmt_val \u001b[38;5;241m=\u001b[39m property_data[elmt_name]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "def write_property_in_sheet(ws:xlsxwriter.worksheet.Worksheet, property_data:Dict, row:int, col:int):\n",
    "    for elmt_name in property_data.keys():\n",
    "        elmt_val = property_data[elmt_name]\n",
    "        ws.write(row, col, elmt_name)\n",
    "        \n",
    "        if not isinstance(elmt_val, List):\n",
    "                ws.write(row, col+1, elmt_val)\n",
    "                row += 1\n",
    "                \n",
    "        else:\n",
    "            for sub_elmt in elmt_val:\n",
    "                if isinstance(sub_elmt, Dict):\n",
    "                    c = 1\n",
    "                    for k in sub_elmt.keys():\n",
    "                        ws.write(row, col+c, sub_elmt[k])\n",
    "                        c += 1\n",
    "                    \n",
    "                else:\n",
    "                    ws.write(row, col+c, sub_elmt)\n",
    "                row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bpu_in_workbook(wb:xlsxwriter.workbook.Workbook, bpu_data:Dict):\n",
    "    worksheet = wb.add_worksheet(\"offre financiere\")\n",
    "    row, col = 0, 0\n",
    "    for k in bpu_data.keys():\n",
    "        worksheet.write(row, col, k)\n",
    "        worksheet.write(row, col+1, bpu_data[k])\n",
    "        row+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AO\n",
    "\n",
    "num_dos = \"2023DOS0550696\"\n",
    "entreprise = \"neurones_JMB\"\n",
    "\n",
    "# open excel file\n",
    "out = f\"/workspace/notebooks/outputs/{num_dos}/{entreprise}/ao.xlsx\"\n",
    "workbook = xlsxwriter.Workbook(out)\n",
    "\n",
    "# read json\n",
    "rec_path = f\"/workspace/notebooks/json/{num_dos}/{entreprise}/formatted_json_rec.json\"\n",
    "with open(rec_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# keys of json ['diplomes', 'certifications', 'langues_parlees', 'experiences_entreprise']\n",
    "# properties_names = list(data.keys())\n",
    "worksheet = workbook.add_worksheet(\"REC et CDC\")\n",
    "row, col = 0, 0\n",
    "for elmt_name in data.keys():\n",
    "    \n",
    "    elmt_val = data[elmt_name]\n",
    "    worksheet.write(row, col, elmt_name)\n",
    "    \n",
    "    if not isinstance(elmt_val, List):\n",
    "            worksheet.write(row, col+1, elmt_val)\n",
    "            row += 1\n",
    "            \n",
    "    else:\n",
    "        for sub_elmt in elmt_val:\n",
    "            if isinstance(sub_elmt, Dict):\n",
    "                c = 1\n",
    "                for k in sub_elmt.keys():\n",
    "                    worksheet.write(row, col+c, sub_elmt[k])\n",
    "                    c += 1\n",
    "                \n",
    "            else:\n",
    "                worksheet.write(row, col+c, sub_elmt)\n",
    "            row += 1\n",
    "\n",
    "row += 4\n",
    "\n",
    "# read json cdc\n",
    "cdc_path = f\"/workspace/notebooks/json/{num_dos}/{entreprise}/formatted_json_cdc.json\"\n",
    "with open(cdc_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for elmt_name in data.keys():\n",
    "    \n",
    "    elmt_val = data[elmt_name]\n",
    "    worksheet.write(row, col, elmt_name)\n",
    "    \n",
    "    if not isinstance(elmt_val, List):\n",
    "            worksheet.write(row, col+1, elmt_val)\n",
    "            row += 1\n",
    "            \n",
    "    else:\n",
    "        for sub_elmt in elmt_val:\n",
    "            if isinstance(sub_elmt, Dict):\n",
    "                c = 1\n",
    "                for k in sub_elmt.keys():\n",
    "                    worksheet.write(row, col+c, sub_elmt[k])\n",
    "                    c += 1\n",
    "                \n",
    "            else:\n",
    "                worksheet.write(row, col+1, sub_elmt)\n",
    "            row += 1\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
